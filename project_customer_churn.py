# -*- coding: utf-8 -*-
"""project : customer_churn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19_2oK803ZimkHAIK8TRUL2Q6c8WD-VbM
"""

# LOAD THE DATA
# Importing Libraries

# Utitlity Libraries
import numpy as np
import pandas as pd

# Visualisation Libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Algorithm, Evaluation, and Model Libraries
from sklearn.linear_model import RidgeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import recall_score
from xgboost import XGBClassifier

# Importing Dataset
data = pd.read_csv("/content/Dataset.csv")
# Printing Data
data.head()

data.shape

data.isna().sum()

data.info()

data.describe()

with sns.color_palette("pastel"):
    fig, axes = plt.subplots(2, 3, figsize=(12, 7), sharey=True)
    sns.countplot(x="gender", data=data, ax=axes[0,0])
    sns.countplot(x="SeniorCitizen", data=data, ax=axes[0,1])
    sns.countplot(x="Partner", data=data, ax=axes[0,2])
    sns.countplot(x="Dependents", data=data, ax=axes[1,0])
    sns.countplot(x="PhoneService", data=data, ax=axes[1,1])
    sns.countplot(x="PaperlessBilling", data=data, ax=axes[1,2])

data.hist(bins=30, figsize=(15, 10))
plt.show()

# Correlation matrix
corr_matrix = data.select_dtypes(include=['number']).corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

#Pairplot
sns.pairplot(data, hue='Churn')
plt.show()

#DATA CLEANING
churnnum = {'Yes':1, 'No':0}
data.Churn.replace(churnnum, inplace=True)
churnnum
genderval = pd.pivot_table(data, values='Churn', index=['gender'],
                    columns=['SeniorCitizen'], aggfunc=np.mean)
genderval
data.drop(['customerID','gender','PhoneService','Contract','TotalCharges'], axis=1, inplace=True)
data.columns

# Resampling
X=data.copy()
nox = X[X.Churn == 0]
noy = X[X.Churn == 1]

yesupsampled = noy.sample(n=len(nox), replace=True, random_state=42)
print(len(yesupsampled))

yupsampled=pd.concat([nox, yesupsampled],axis=0)

# FEATURE ENGINEERING
!pip install scikit-learn # Install scikit-learn if you haven't already
from sklearn.preprocessing import MinMaxScaler # Import MinMaxScaler

minmax = MinMaxScaler()
a = minmax.fit_transform(data[['tenure']])
b = minmax.fit_transform(data[['MonthlyCharges']])
X['tenure'] = a
X['MonthlyCharges'] = b

X.shape

# DATASET SPLITTING
from sklearn.model_selection import train_test_split
X = yupsampled.drop(['Churn'], axis=1) #features (independent variables)
y = yupsampled['Churn'] #target (dependent variable)

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X)  # This will convert categorical columns to numerical

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state=50)

#MODEL TRAINING, FITTING, TRAINING & EVALUATION
# Ridge Classifier

# Ceeating a model
ridgeclassifier = RidgeClassifier()

# Fitting Classifier
ridgeclassifier.fit(X_train, y_train)

# Predicting training values
pred = ridgeclassifier.predict(X_train)

# Getting accuracy
accuracy_score(y_train, pred)

# Predicting testing values
pred_test = ridgeclassifier.predict(X_test)

# Printing accuracy
accuracy_score(y_test, pred_test)

# Random Forest

# Creating Model
randomforest = RandomForestClassifier(n_estimators=250, max_depth=18)

# Fitting Model
randomforest.fit(X_train, y_train)

# Pridicting training accuracy
pred = randomforest.predict(X_train)

# Printing training accuracy
accuracy_score(y_train, pred)

# Predicting testing accuracy
pred_test = randomforest.predict(X_test)

# Printing testing score
accuracy_score(y_test, pred_test)

# Grid Search

# Setting parameters
parameters = {'n_estimators':[150,200,250,300], 'max_depth':[15,20,25]}

# Using Grid Search with Random Forest Classifier
forest = RandomForestClassifier()
clf = GridSearchCV(estimator=forest, param_grid=parameters, n_jobs=-1, cv=5)

# Fitting the model
clf.fit(X, y)

# Grid Search To Get Best Hyperparameters
parameters = {'C':[0.01,0.1,1,3,5,10]}
svmclf = SVC(class_weight='balanced',random_state=43)
grid = GridSearchCV(estimator=svmclf, param_grid=parameters,scoring='accuracy',return_train_score=True,verbose=1)
grid.fit(X_train,y_train)

# Plotting the values
cv_result = pd.DataFrame(grid.cv_results_)
plt.scatter(cv_result['param_C'],cv_result['mean_train_score'])
plt.plot(cv_result['param_C'],cv_result['mean_train_score'],label='Train')
plt.scatter(cv_result['param_C'],cv_result['mean_test_score'])
plt.plot(cv_result['param_C'],cv_result['mean_test_score'],label="CV")
plt.title('Hyperparameter vs accuracy')
plt.legend()
plt.xlabel('C')
plt.ylabel('Accuracy')
plt.show()

# Getting the best parameters and score
clf.best_params_
{'max_depth': 20, 'n_estimators': 150}
clf.best_score_

# Training the model using the optimal parameters discovered with SVM Classifier
svmclf =  SVC(C=3,class_weight='balanced', random_state=43)
svmclf.fit(X_train,y_train)

result2 = ["2.","SVM","Balanced using class weights"]
y_pred_tr = svmclf.predict(X_train)
print('Train accuracy SVM: ',accuracy_score(y_train,y_pred_tr))
result2.append(round(accuracy_score(y_train,y_pred_tr),2))

y_pred_test = svmclf.predict(X_test)
print('Test accuracy SVM: ',accuracy_score(y_test,y_pred_test))
result2.append(round(accuracy_score(y_test,y_pred_test),2))

recall = recall_score(y_test,y_pred_test)
print("Recall Score: ",recall)
result2.append(round(recall,2))

# Building a confusion matrix
matrix = confusion_matrix(y_test,y_pred_test)
ax=plt.subplot();
sns.heatmap(matrix, annot=True, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
ax.set_ylim(2.0,0)
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(['Neg','Pos'])
ax.yaxis.set_ticklabels(['Neg','Pos'])
plt.show()

import math
scale=round(math.sqrt(y_train.value_counts()[0]/y_train.value_counts()[1]),2)
# Grid Search To Get Best Hyperparameters
parameters = {"learning_rate"    : [0.10,0.20,0.30 ],\
              "max_depth"        : [ 3,5,10,20],\
              "n_estimators" : [ 100, 200, 300, 500],\
              "colsample_bytree" : [ 0.3, 0.5, 0.7 ] }
clf_xgb = XGBClassifier(scale_pos_weight=scale, eval_metric ='mlogloss')
grid = GridSearchCV(estimator=clf_xgb, param_grid=parameters, scoring='accuracy',return_train_score=True,verbose=1)
grid.fit(X_train,y_train)

# plotting only the first 70 train scores
cv_result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_train_score',ascending=True)[:70]
param_list = list(cv_result['params'])
param_index = np.arange(70)
plt.figure(figsize=(18,6))
plt.scatter(param_index,cv_result['mean_train_score'])
plt.plot(param_index,cv_result['mean_train_score'],label='Train')
plt.scatter(param_index,cv_result['mean_test_score'])
plt.plot(param_index,cv_result['mean_test_score'],label="CV")
plt.title('Hyperparameter vs accuracy')
plt.grid()
plt.legend()
plt.xlabel('Hyperparametr combination Dict')
plt.ylabel('Accuracy')
plt.show()

best_parameters = param_list[34]
print(best_parameters)

# Using XG Boost
clf_xgb = XGBClassifier(learning_rate= best_parameters['learning_rate'] ,max_depth=best_parameters ['max_depth'], n_estimators=best_parameters['n_estimators'], colsample_bytree=best_parameters['colsample_bytree'],                        eval_metric='mlogloss',scale_pos_weight=scale)
clf_xgb.fit(X_train,y_train)

xgbresult = ["4.","XGBClassifier","Balanced using scale_pos_weight"]
y_pred_tr = clf_xgb.predict(X_train)
print('Train accuracy XGB: ',accuracy_score(y_train,y_pred_tr))
xgbresult.append(round(accuracy_score(y_train,y_pred_tr),2))

# Use X_test here instead of X_train
y_pred_test = clf_xgb.predict(X_test)
print('Test accuracy XGB: ',accuracy_score(y_test,y_pred_test))
xgbresult.append(round(accuracy_score(y_test,y_pred_test),2))

recall = recall_score(y_test,y_pred_test)
print("Recall Score: ",recall)
xgbresult.append(round(recall,2))

# Building confusion matrix
cm = confusion_matrix(y_test,y_pred_test)
ax=plt.subplot();
sns.heatmap(cm, annot=True, fmt='d', linewidths=2, linecolor='black', cmap='YlGnBu',ax=ax)
ax.set_xlabel('Predicted')
ax.set_ylabel('Actual')
ax.set_ylim(2.0,0)
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(['Neg','Pos'])
ax.yaxis.set_ticklabels(['Neg','Pos'])
plt.show()

#CONCLUSION
# Customer Churn Prediction Project determines when a customer is about to leave the organization.
# Random Forest and XGBoost give us the best accuracy in this situation.

